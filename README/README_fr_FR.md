[![English](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-usa2x.png)](/README.md)
[![Fran√ßais](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-france2x.png)](/README/README_fr_FR.md)
[![‰∏≠Êñá](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-china2x.png)](/README/README_zh_CN.md)
[![Êó•Êú¨Ë™û](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-japan2x.png)](/README/README_ja_JP.md)

# Comprendre les sc√®nes visuelles √† l'aide de r√©seaux de neurones √† tenseur logistique üöÄü§ñ

[![Python 3.12](https://img.shields.io/badge/Python-3.12-blue?style=flat-square)](https://www.python.org)
[![CUDA 12.4](https://img.shields.io/badge/CUDA-12.4-red?style=flat-square)](https://developer.nvidia.com/cuda-toolkit)
[![LTNTorch](https://img.shields.io/badge/Project-LTNTorch-9cf?style=flat-square)](https://github.com/tommasocarraro/LTNtorch)
[![Visual Genome](https://img.shields.io/badge/Data-Visual%20Genome-yellow?style=flat-square)](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html)
[![YOLO](https://img.shields.io/badge/Detection-YOLO-orange?style=flat-square)](https://github.com/ultralytics/ultralytics)
[![OneFormer](https://img.shields.io/badge/Segmentation-OneFormer-brightgreen?style=flat-square)](https://github.com/SHI-Labs/OneFormer)

Ce projet combine un mod√®le de segmentation et un r√©seau tensoriel logique pour r√©aliser le raisonnement de la relation entre les objets dans les images et am√©liorer l'analyse du contenu de l'image gr√¢ce √† une formule logique de premier ordre et √† un r√©seau perceptron multicouche. par le biais d'une formule logique de premier ordre et d'un r√©seau perceptron multicouche. ‚ú®

---

## Architecture g√©n√©rale et r√©partition des modules
![Architecture g√©n√©rale](/README/images/Architecture.png)

1) **‚ú® Segmentation de l'image et extraction des caract√©ristiques** : Le mod√®le YOLO-Seg de [UltraLytics](https://docs.ultralytics.com) ou le mod√®le OneFormer de [SHI-Labs](https://www.shi-labs.com) est utilis√© pour la segmentation de l'image d'entr√©e et l'extraction des caract√©ristiques. L'image d'entr√©e est utilis√©e pour la segmentation et l'extraction des caract√©ristiques.
2. **‚ú®Goal relation detection** : En utilisant un r√©seau tensoriel logique de [LTNTorch](https://github.com/tommasocarraro/LTNtorch), chaque objectif est converti en un pr√©dicat logique, qui est ensuite raisonn√© par le r√©seau tensoriel logique.
3) **‚ú®Logical Relationship Training** : Les r√©seaux de tenseurs logistiques ont √©t√© entra√Æn√©s √† l'aide de donn√©es relationnelles provenant de la base de donn√©es [Visual Genome](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html).
4. **‚ú® Sortie des r√©sultats du raisonnement** : lit les relations trouv√©es par l'utilisateur en utilisant la forme d'un ternaire et sort les r√©sultats du raisonnement.


## Guide d'installation

### Environnement de formation (Ubuntu 22.04)
```bash
pip install -r requirements.train.txt
```

### Environnement de raisonnement (macOS 15.3)
```bash
pip install -r requirements.inference.txt
```

Des mod√®les pr√©-entra√Æn√©s pour YOLO et OneFormer sont automatiquement t√©l√©charg√©s lors de l'ex√©cution du programme.

## Lignes directrices pour l'utilisation

### Exemple de formation
```Python
from utils.Trainer import trainer

predicate = ["in", "on", "next to"]
for pred in predicate:
    print(f"üöÇ Entra√Æner {pred} ...")
    trainer(
        pos_predicate=pred,
        neg_predicates=[p for p in predicate if p != pred],
        epoches=50,
        batch_size=32,
        lr=1e-4
    )
```

### Exemples de raisonnement
```Python
from utils.Inferencer import Inferencer

# Initialiser l'inf√©renteur
analyzer = Inferencer(
    subj_class="person",
    obj_class="bicycle",
    predicate="near"
)

# Effectuer l'inf√©rence sur une seule image
result = analyzer.inference_single("demo.jpg")
print(f"üîé Obtenu Ôºö{result['relation']} (ConfianceÔºö{result['confidence']:.2f})")

# Effectuer une inf√©rence sur un dossier d'images
analyzer.process_folder("input_images/")
```

# Base de donn√©es
Les relations et les m√©tadonn√©es des images de la base de donn√©es [Visual Genome](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html) ont √©t√© utilis√©es pour extraire les informations sur les images et les paires de caract√©ristiques. ont √©t√© utilis√©es pour extraire des informations sur les images et des informations sur les paires de caract√©ristiques.

![Visual Genole Example](/README/images/Visual_Genome.png)

Le projet extrait les donn√©es et les emplacements cibles √† partir de donn√©es relationnelles et extrait les donn√©es d'image pour normaliser les emplacements cibles.

# Code Style and Documentation
This project uses the ```black``` and ```isort``` to automatically enforce a consistent code style. All code comments and documentation follow the [Google Python Style Guide](https://google.github.io/styleguide/) to maintain clarity and consistency.


Use the following command to keep the code in the same format before submitting.
```bash
black . && isort .
```
# Acknowledgements
This project is based on the [LTNTorch](https://github.com/tommasocarraro/LTNtorch) project and uses the [Visual Genome](https://homes.cs.washington.edu/~ranjay/visualgenome/api_beginners_tutorial.html) database for data extraction. The project uses the [YOLO](https://doc.ultralytics.com) and [OneFormer](https://www.shi-labs.com) models for object detection and segmentation.

# License
This project is licensed under the GNU3.0 License - see the [LICENSE](/LICENSE) file for details.
---