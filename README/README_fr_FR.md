[![English](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-usa2x.png)](/README.md)
[![Fran√ßais](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-france2x.png)](/README/README_fr_FR.md)
[![‰∏≠Êñá](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-china2x.png)](/README/README_zh_CN.md)
[![Êó•Êú¨Ë™û](https://cdn3.iconfinder.com/data/icons/142-mini-country-flags-16x16px/32/flag-japan2x.png)](/README/README_ja_JP.md)

# Comprendre les sc√®nes visuelles √† l'aide de r√©seaux de neurones √† tenseur logistique üöÄü§ñü§ñ

[![Python 3.12](https://img.shields.io/badge/Python-3.12-blue?style=flat-square)](https://www.python.org)
[![CUDA 12.4](https://img.shields.io/badge/CUDA-12.4-red?style=flat-square)](https://developer.nvidia.com/cuda-toolkit)
[![LTNTorch](https://img.shields.io/badge/Project-LTNTorch-9cf?style=flat-square)](https://github.com/tommasocarraro/LTNtorch)
[![Visual Genome](https://img.shields.io/badge/Data-Visual%20Genome-yellow?style=flat-square)](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html)
[![YOLO](https://img.shields.io/badge/Detection-YOLO-orange?style=flat-square)](https://github.com/ultralytics/ultralytics)
[![OneFormer](https://img.shields.io/badge/Segmentation-OneFormer-brightgreen?style=flat-square)](https://github.com/SHI-Labs/OneFormer)

Ce projet combine un mod√®le de segmentation et un r√©seau tensoriel logique pour r√©aliser le raisonnement de la relation entre les objets dans les images et am√©liorer l'analyse du contenu de l'image gr√¢ce √† une formule logique de premier ordre et √† un r√©seau perceptron multicouche. par le biais d'une formule logique de premier ordre et d'un r√©seau perceptron multicouche. ‚ú®

---

## Architecture technique

1. **Segmentation et extraction de caract√©ristiques** : en utilisant le formulaire YOLO [UltraLytics](https://docs.ultralytics.com) et le formulaire OneFormer [SHI-Labs](https://www.shi-labs.com)
2. **Criblage des objets** : conservation des objets pr√©sentant un int√©r√™t
3. **G√©n√©ration de tenseurs logiques** : produit cart√©sien de paires d'objets pour g√©n√©rer un tenseur logique
4. **Raisonnement logique** : raisonnement par pr√©dicats relationnels √† l'aide du tenseur logique
5) **Sortie des r√©sultats** : sortie des r√©sultats du raisonnement


## Guide d'installation

### Environnement de formation (Ubuntu 22.04)
```bash
pip install -r requirements.train.txt
```

### Environnement de raisonnement (macOS 15.3)
```bash
pip install -r requirements.inference.txt
```

Des mod√®les pr√©-entra√Æn√©s pour YOLO et OneFormer sont automatiquement t√©l√©charg√©s lors de l'ex√©cution du programme.

## Lignes directrices pour l'utilisation

### Exemple de formation
```Python
from utils.Trainer import trainer

predicate = ["in", "on", "next to"]
for pred in predicate:
    print(f"üöÇ Entra√Æner {pred} ...")
    trainer(
        pos_predicate=pred,
        neg_predicates=[p for p in predicate if p != pred],
        epoches=50,
        batch_size=32,
        lr=1e-4
    )
```

### Exemples de raisonnement
```Python
from utils.Inferencer import Inferencer

# Initialiser l'inf√©renteur
analyzer = Inferencer(
    subj_class="person",
    obj_class="bicycle",
    predicate="near"
)

# Effectuer l'inf√©rence sur une seule image
result = analyzer.inference_single("demo.jpg")
print(f"üîé Obtenu Ôºö{result['relation']} (ConfianceÔºö{result['confidence']:.2f})")

# Effectuer une inf√©rence sur un dossier d'images
analyzer.process_folder("input_images/")
```

# Base de donn√©es
Les relations et les m√©tadonn√©es des images de la base de donn√©es [Visual Genome](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html) ont √©t√© utilis√©es pour extraire les informations sur les images et les paires de caract√©ristiques. ont √©t√© utilis√©es pour extraire des informations sur les images et des informations sur les paires de caract√©ristiques.

![Visual Genole Example](/README/images/Visual_Genome.png)

Le projet extrait les donn√©es et les emplacements cibles √† partir de donn√©es relationnelles et extrait les donn√©es d'image pour normaliser les emplacements cibles.

# Code Style and Documentation
This project uses the ```black``` and ```isort``` to automatically enforce a consistent code style. All code comments and documentation follow the [Google Python Style Guide](https://google.github.io/styleguide/) to maintain clarity and consistency.


Use the following command to keep the code in the same format before submitting.
```bash
black . && isort .
```
# Acknowledgements
This project is based on the [LTNTorch](https://github.com/tommasocarraro/LTNtorch) project and uses the [Visual Genome](https://homes.cs.washington.edu/~ranjay/visualgenome/api_beginners_tutorial.html) database for data extraction. The project uses the [YOLO](https://doc.ultralytics.com) and [OneFormer](https://www.shi-labs.com) models for object detection and segmentation.

# License
This project is licensed under the GNU3.0 License - see the [LICENSE](/LICENSE) file for details.
---